---
title: "计算机基础：分布式系统（Part 5/6）"
excerpt: '三大分布式问题'

collection: theory
category: basic
permalink: /theory/basic/ds
tags: 
  - distributed

layout: single
read_time: true
author_profile: false
comments: true
share: true
related: true
---

![](../../images/theory/basic/ds/cap.png)

## 一致性问题

### 1. 理论基础

**CAP定理**
- **核心内容**：分布式系统最多同时满足一致性(C)、可用性(A)、分区容错性(P)中的两项
- **实践解读**：
  - 不是"三选二"[^1]，而是"网络分区发生时必须在C和A之间选择"
  - 现代系统往往按工作负载分区，而不是整体选择CAP位置
- **关键洞察**：没有完美的解决方案，只有针对场景的权衡

**FLP不可能定理**
- **定理陈述**：在异步分布式系统中，即使只有一个进程可能崩溃，也不存在确定性算法能在有限时间内达成共识
- **实践意义**：
  - 打破了"完美容错共识"的幻想
  - 迫使系统设计者接受弱化条件：随机化算法、故障检测器、部分同步假设
- **与CAP的关系**：FLP从算法层面证明共识的困难，CAP从系统属性层面定义设计约束

### 2. 一致性模型

| 模型 | 定义 | 保证 | 典型应用 |
|------|------|------|----------|
| **强一致性** | 所有操作看起来像是原子地执行，且立即全局可见 | 任何读取都返回最新写入 | 数据库事务、ZooKeeper |
| **顺序一致性** | 所有进程看到的操作顺序一致，但不必实时 | 全局一致的顺序 | 分布式共享内存 |
| **因果一致性** | 保持因果关系的操作有序，并发操作顺序任意 | 因果关系不被破坏 | 分布式社交网络 |
| **最终一致性** | 在没有新更新的情况下，最终所有访问返回相同值 | 最终会收敛到一致状态 | DNS、Cassandra |

### 3. 共识算法

共识是解决强一致性的核心机制。

**Paxos家族**
- **Basic Paxos**：解决单值共识
  - 角色：Proposer、Acceptor、Learner
  - 阶段：Prepare阶段、Accept阶段
  - 核心：多数派原则、提案编号递增

- **Multi-Paxos**：连续多值共识的优化
  - 选举稳定的Leader，跳过Prepare阶段
  - 实际系统基础（Chubby、Spanner的基础）

**Raft算法**
- **设计目标**：可理解性、可实现性
- **核心分解**：
  1. **领导选举**：Term概念、随机超时、多数派投票
  2. **日志复制**：领导者接受请求，复制到多数派后提交
  3. **安全性**：选举限制（日志必须足够新）、提交规则

- **关键机制**：
  - 任期（Term）：逻辑时钟，保证单调递增
  - 日志匹配特性：如果两个日志条目有相同的索引和任期，则内容相同
  - 领导者完全性：拥有最新已提交日志的节点才能成为领导者

**拜占庭容错共识**
- **PBFT**：实用拜占庭容错
  - 三阶段协议：Pre-prepare、Prepare、Commit
  - 容错能力：可容忍f个恶意节点，需要3f+1个节点
- **区块链共识**：PoW、PoS、DPoS等
  - 解决开放网络中的信任问题
  - 代价：性能较低、能耗较高

### 4. 复制策略

**主从复制（Leader-Follower）**
```python
# 典型主从复制流程
def write_operation(data):
    # 1. 客户端写入主节点
    leader.write(data)
    
    # 2. 主节点同步到从节点（同步/异步）
    for follower in followers:
        follower.replicate(data)
    
    # 3. 主节点确认写入成功
    return success
```

- **同步复制**：强一致，但延迟高、可用性低
- **异步复制**：高可用，但可能丢失数据
- **半同步复制**：折中方案，部分从节点同步确认

**多主复制**
- **优点**：写可用性高、延迟低（就近写入）
- **挑战**：写冲突解决（最后写入胜出、自定义合并逻辑）
- **应用**：多数据中心部署、离线应用同步

**无主复制**
- **Dynamo风格**：客户端直接写入多个副本
- **读修复**：读取时发现数据旧，触发更新
- **反熵**：后台进程同步数据差异
- **Quorum机制**：R+W > N保证强一致

### 5. 设计哲学：ACID vs BASE

**ACID（传统数据库哲学）**
- **Atomicity**：事务要么全做，要么全不做
- **Consistency**：事务使数据库从一个一致状态到另一个一致状态
- **Isolation**：并发事务互不干扰
- **Durability**：事务提交后永久保存

**BASE（互联网系统哲学）**
- **Basically Available**：基本可用，而非完全可用
- **Soft State**：允许中间状态，不一定时刻一致
- **Eventually Consistent**：最终一致

## 容错问题

### 1. 故障模型

理解可能出错的地方，故障分类：
- 节点故障，机器宕机、进程崩溃（相对简单）
- 网络故障，
  - 网络丢包/延迟（部分故障，可缓解）
  - 网络分区（完全故障，最具破坏性），引发脑裂。

### 2. 冗余设计

故障的预防。

**数据冗余**
- **复制**
  - 完全复制，强一致性
  - 部分复制，最终一致性
- **分片**
  - 范围分片
  - 哈希分片
  - 一致性哈希
- **纠删码**

**服务冗余**
- **无状态服务**：易于水平扩展，实例故障无影响
- **有状态服务**：需要状态同步或外部存储
- **部署模式**：N+1冗余（多一个备用）、N+2冗余

### 3. 故障处理

检测→隔离→恢复

**故障的检测**
- **心跳机制**：定期发送心跳，超时判断故障
- **租约机制**：有时间限制的资源锁，过期自动释放
- **φ-累积故障检测器**：自适应超时，考虑网络状况

**故障的隔离，熔断降级**
- **功能降级**：关闭非核心功能（如关闭推荐，保留购物）
- **数据降级**：返回缓存数据或默认值
- **服务降级**：排队、限流、拒绝部分请求

### 4. 幂等性设计

**为什么需要幂等性**
- 网络可能重复发送请求
- 客户端可能超时重试
- 故障恢复后需要重新执行

**实现模式**
| 模式 | 实现方式 | 适用场景 |
|------|----------|----------|
| **唯一标识** | 每个请求有唯一ID，服务端去重 | HTTP API、消息队列 |
| **版本控制** | 数据版本号，拒绝旧版本更新 | 数据存储、状态机 |
| **状态机** | 操作基于当前状态，重复操作无影响 | 工作流、订单系统 |

### 5. 高可用架构模式

**主从模式**
- **主动-被动**：主节点服务，从节点热备
- **故障转移**：主节点故障时自动切换
- **挑战**：数据同步延迟、脑裂问题

**多活模式**
- **同城多活**：机房级别冗余，延迟低
- **异地多活**：地理分布，容灾能力强
- **全球多活**：用户就近访问，体验好

**无单点架构**
- ...

## 并发问题

### 1. 分布式锁

 **锁的需求场景**
1. 资源互斥访问（如配置更新）
2. 防止重复处理（如定时任务）
3. 顺序控制（如选主）

 **实现方式对比**
| 方案 | 优点 | 缺点 | 适用场景 |
|------|------|------|----------|
| **基于数据库** | 简单，利用现有设施 | 性能差，单点风险 | 低频操作，已有数据库 |
| **基于Redis** | 性能高，丰富数据结构 | 非强一致，需处理锁续期 | 高并发，可接受偶尔错误 |
| **基于ZooKeeper** | 强一致，临时节点自动清理 | 性能中等，依赖ZooKeeper集群 | 强一致需求，协调任务 |
| **基于etcd** | 强一致，高可用，租约机制 | 相对较新，生态不如ZK成熟 | Kubernetes环境，强一致需求 |

 **RedLock算法争议**
- **目标**：在Redis集群中实现分布式锁
- **算法步骤**：
  1. 获取当前时间
  2. 轮流尝试在N个节点上加锁
  3. 计算加锁耗时，如果耗时小于锁超时时间且获得多数锁，则成功
- **争议点**：时钟跳跃、GC停顿可能破坏安全性

### 2. 分布式事务

**两阶段提交（2PC）**
- **阶段1：准备阶段**
  - 协调者询问所有参与者是否可提交
  - 参与者执行事务，写redo/undo日志，但不提交
- **阶段2：提交/回滚阶段**
  - 协调者根据参与者响应决定提交或回滚
  - 通知所有参与者执行最终操作
- **问题**：
  - 同步阻塞：参与者等待协调者
  - 单点故障：协调者故障导致事务阻塞
  - 数据不一致：部分参与者收到提交，部分未收到

**三阶段提交（3PC）**
- 增加CanCommit阶段，减少阻塞时间
- 超时机制：参与者超时自动提交/回滚
- 仍然无法完全解决一致性问题

**Saga模式**
- **核心思想**：长事务拆分为多个本地事务
- **补偿事务**：每个正向事务对应一个补偿操作
- **两种协调方式**：
  - 编排式：每个服务触发下一个服务
  - 协同式：协调者调度所有服务

**TCC模式（Try-Confirm-Cancel）**
- **Try阶段**：预留资源（如冻结库存）
- **Confirm阶段**：确认执行，使用预留资源
- **Cancel阶段**：取消执行，释放预留资源

**优点**：业务自主控制，资源锁定时间短
**缺点**：业务侵入性强，需要实现三个接口

**AT模式**
- 对经典的两阶段提交（2PC）进行了根本性改造
- 第一阶段：直接提交本地事务。在执行业务SQL时，Seata会通过数据源代理自动拦截，解析SQL，在事务提交前保存数据更新前后的镜像（before_image和after_image），生成回滚日志（undo_log），并连同业务数据一起提交到本地数据库。此时，数据已真实提交并释放本地锁。
- 第二阶段：
  - 若全局事务成功，则异步清理一阶段生成的回滚日志即可。
  - 若全局事务需要回滚，则根据一阶段保存的回滚日志，生成反向的补偿SQL（即“回滚操作”）来还原数据。

**本地消息表**
- **核心**：利用本地事务保证消息可靠投递
- **步骤**：
  1. 执行本地事务，插入业务消息到本地消息表
  2. 定时任务轮询消息表，发送未处理消息
  3. 消费方处理消息，回调确认

### 3. 并发控制机制

**悲观并发控制**
- **两阶段锁（2PL）**
  - 增长阶段：只能获取锁，不能释放
  - 缩减阶段：只能释放锁，不能获取
- **实现简单**，但并发度低，可能死锁

**乐观并发控制（OCC）**
- **三阶段**：读阶段、验证阶段、写阶段
- **基于版本/时间戳**：检查数据是否被修改
- **适用场景**：读多写少，冲突较少

**多版本并发控制（MVCC）**
- **核心**：为数据维护多个版本
- **读写不阻塞**：读读读旧版本，写写写新版本
- **实现**：
  - 版本号或时间戳标识版本
  - 垃圾回收清理旧版本
- **应用**：MySQL InnoDB、PostgreSQL、Oracle

**无冲突复制数据类型（CRDT）**
- **数学保证**：无论操作顺序，最终收敛到相同状态
- **两种类型**：
  - 基于状态：传递完整状态，merge函数满足交换律、结合律、幂等律
  - 基于操作：传递操作，操作满足交换性
- **适用场景**：协同编辑、计数器、集合操作

[^1]: DeepSeek AI辅助生成
