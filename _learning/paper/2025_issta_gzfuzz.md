---
title: "Gzfuzz: 基于强化学习的Gazebo模糊测试工具"
excerpt: 'ISSTA 2025, Reinforcement Learning-Based Fuzz Testing for the Gazebo Robotic Simulator'

collection: learning
category: paper
permalink: /learning/2025-issta-gzfuzz
tags: 
  - issta
  - fuzz
  - gazebo

layout: single
read_time: true
author_profile: false
comments: true
share: true
related: true
---

![](../images/learning/2025_issta_gzfuzz/cover.png)

## Gazebo 

### 一、应用场景

Gazebo是使用最广泛的机器人模拟器，为在高保真 3D 环境中开发、测试和验证机器人系统提供了一个强大而灵活的平台，是 ROS2 的默认模拟器。其应用范围广泛，例如 NASA 太空任务，DARPA 地下探索和自动驾驶。

与其他复杂的软件系统一样，Gazebo 容易出现可能产生严重后果的错误，通过模拟不准确、测试失败和崩溃来影响机器人开发。例如，在数字孪生系统中，模拟器崩溃可能破坏虚拟和物理模型之间的同步，导致意外停机或操作问题。

### 二、模糊测试的挑战

直接将现有的这些模糊测试方法移植到 Gazebo 可能不会产生令人满意的结果。原因在于 Gazebo 使用复杂的输入格式和基于插件的复杂客户端-服务器架构来实现实时物理计算，如下图所示。

![](../images/learning/2025_issta_gzfuzz/gazebo.png)

这些特性使得标准模糊测试工具难以有效探索 Gazebo 中的潜在错误范围。特别是，我们确定了两个主要挑战，描述如下：
- 严格输入挑战（语法语义限制）：Gazebo 服务器的输入有两种形式，模拟描述格式(SDF) 文件和外部命令。SDF 文件使用基于 XML 的语法来定义用于机器人模拟、可视化和控制的对象和环境，而外部命令利用 Protobuf 规范与服务器通信。输入必须遵循严格的语法和语义规则才能与模拟交互。例如，生成机器人的命令必须包含有效的机器人模型、定位数据以及其他参数（如物理属性）。
- 状态空间挑战（代码路径限制）：Gazebo 的模拟过程可能具有高度复杂和庞大的状态空间，涉及使用插件的不同模型、关节、传感器和物理交互，例如，用于控制机器人、模拟摄像头和 LiDAR 传感器以及与环境交互的插件。插件和其他组件共同工作以模拟现实世界的交互，但它们的复杂性产生了巨大的状态空间。

### 三、应对挑战

为了应对这些挑战，本文提出了第一个 GaZebo fuzzing 方法 GzFuzz：
- 利用语法感知的可行命令生成机制，确保模糊测试的输入，特别是 Gazebo 命令，在模拟环境中语法上是有效的。
- 基于学习的命令生成器选择机制，该机制基于多维反馈来指导模糊测试过程。通过使用强化学习，GzFuzz 学习哪些类型的命令或命令序列更有可能探索 Gazebo 模拟环境中未经测试或探索不足的状态。

## GzFuzz

GzFuzz开源在[github](https://github.com/liyitao-code/GzFuzz)。

### 一、框架

GzFuzz 的工作流程如下图所示。

![](../images/learning/2025_issta_gzfuzz/gzfuzz.png)

给定一个 SDF 文件，模糊测试过程如下工作：
- 1.提取 SDF 文件的特征。
- 2-4.使用强化学习框架生成一系列命令生成器选择。
- 5-6.基于这些选择，借助挖掘的模型、插件和 Protobuf 定义，将 Gazebo 命令构建为客户端命令行字符串。
- 7.生成一个 Gazebo 服务器进程，并执行生成的命令与服务器通信。执行后，我们能够收集测试结果。
- 8.分析测试结果，并相应地更新强化学习模块。

关于测试预言，我们主要关注与崩溃相关的错误，因为它们的严重性、清晰性和可重现性。

### 二、实现

#### 1.预处理

为了实现模糊测试过程，初始步骤是收集一组种子 SDF 文件。在预处理阶段，我们首先遍历 Gazebo 的源代码仓库，并提取了总共 309 个 SDF 文件用于分析和模糊测试目的。然后，我们使用 [lxml](https://pypi.org/project/lxml) 库将所有 SDF 文件解析为文档对象模型 (DOM) 树。更具体地说，Gazebo 区分两种类型的插件：附加到 world 节点的插件和附加到 model 节点的插件。我们从 SDF 文件中提取这两种类型的插件，总共收集了 117 个插件。

在下图中，我们展示了一个 SDF 文件示例、其解析后的 DOM 树以及在 Gazebo 中渲染的场景。

![](../images/learning/2025_issta_gzfuzz/sdf.png)

在 SDF 文件中，\<world\> 节点封装了整个环境。在 world 内部，\<model\> 节点代表单个对象或机器人，它们由 \<link\> 节点组成，定义了物理组件，如底盘或车轮。这些链接通过 \<joint\> 节点相互连接，允许机械关节连接。此外，\<plugin\> 节点提供了注入自定义功能的能力，例如用于控制车辆转向的 `gz-sim-ackermann-steering-system`。

预处理步骤确保我们在模糊测试过程中拥有多样化的模型和插件集，用于生成有意义且有效的命令。在本研究中，我们考虑使用外部命令对 Gazebo 进行模糊测试，而**没有直接变异SDF文件**，出于以下考虑：
- 首先，通过外部命令进行模糊测试允许与 Gazebo 服务器进行更动态的交互，在自动化范式中模拟真实世界的使用场景。与在模拟启动时加载的静态 SDF 文件不同，服务和主题提供了一种连续、实时的操纵模拟的方法，从而带来更即时和多样化的反馈。
- 其次，也可以通过调用 `/world/.../create` 服务来对 SDF 文件进行模糊测试，该服务允许在模拟期间传入不同的 SDF 内容。

#### 2.语法感知的可行命令生成

必须确保生成的命令在模拟环境中语法上是有效的。为了实现这一点，我们根据 Gazebo 命令的特性和预期的测试目标将其分为三个不同的组，即随机生成的命令、半现实命令和破坏性命令。每个类别在发现 Gazebo 中不同类型的问题方面都有特定目的：
- 随机生成的命令：这些命令自动生成以符合 Gazebo 在 Protobuf 格式下的服务和主题要求。目标是探索具有语法有效但不可预测消息的广泛输入空间，帮助发现边缘情况和潜在的系统崩溃。以随机服务为例，命令生成如下：
1. 首先，使用命令 `gz service -l` 获取候选服务。
2. 然后，随机选择一个服务名称 SN，并通过 `gz service -s SN -i` 检索所需的消息类型。
3. 有了消息类型，就可以使用第三方库 [randomproto](https://pypi.org/project/randomproto)生成消息。
- 半现实命令：此类别模拟典型的用户操作，例如在模拟中添加或删除模型、附加插件或移动对象。例如，要生成一个用于插入带有插件的模型的命令，我们首先在预处理期间准备候选模型。例如，我们可以调用 `/world/.../create` 服务并传入 SDF 模型，以处理带有插件的模型的实际插入。类似地，要生成插入不带插件的原始模型的命令，我们利用 [libsdfformat](https://github.com/gazebosim/sdformat)，并实现链接、碰撞和关节的构建。
- 破坏性命令：在半现实命令的基础上，破坏性命令引入了扰动，例如修改插件参数或发出无效操作（例如，用可能无效的值随机变异插件 DOM 树的叶节点）。

在下图中，我们总结了本研究中设计的命令生成器。所提出的生成器经过精心设计：生成器 1-2 可以通过随机生成的命令覆盖所有可用的服务和主题。生成器 3-7 模拟现实用例，以测试 Gazebo 在正常条件下的表现。生成器 8-10 旨在对 Gazebo 的错误处理能力和处理极端或不正确输入的能力进行压力测试。值得注意的是，生成器 6-9 需要额外的参数来生成相应的命令。例如，生成器 6 用于插入带有插件的模型，其中模型是从 Gazebo 源代码仓库中挖掘的。

![](../images/learning/2025_issta_gzfuzz/generators.png)

#### 3.基于学习的命令生成器选择

如前一节所述，命令生成器可能需要参数，这本质上是在选择生成器之后需要进一步做出的决策。由于模糊测试是一个典型的迭代过程，在迭代 $t$ 时，命令生成器序列表示为 $c_t = { \langle c_{t_1}, p_{t_1} \rangle, \langle c_{t_2}, p_{t_2} \rangle, \dots, \langle c_{t_{n_e}}, p_{t_{n_e}} \rangle }$，其中 $c_{t_i}$ 表示第 $i$ 个命令生成器的生成器索引，$p_{t_i}$ 表示第 $i$ 个生成器的参数索引，$n_e$ 是序列的长度。如果生成器 $c_{t_i}$ 不需要参数，则 $p_{t_i} = \text{None}$。此外，每个序列 $c$ 伴随一个向量 $V$，其中每个元素 $v_i$ 表示第 $i$ 个命令生成器在 $c$ 中的出现次数。

由于需要选择命令生成器及其参数，我们采用分层强化学习 (HRL) 来对 Gazebo 进行模糊测试，因为它能够有效管理复杂的决策过程并探索大型状态空间。

在本研究中，命令生成器选择包括三个组成部分，即一个行动者模块、一个评判者模块和一组子行动者模块。更具体地说，我们采用两级决策策略：一个行动者用于选择命令生成器，一组子行动者用于选择参数，所有这些都由一个共享的评判者模块指导。每个模块都使用全连接神经网络实现，输入层接收特征向量，后面是一个包含 $N$ 个神经元的隐藏层。行动者/子行动者输出生成器或参数上的概率分布，而评判者预测当前输入的标量奖励值。

对于行动者、子行动者和评判者模块，输入是从 SDF 文件中提取的特征向量，主要通过捕获 Gazebo 模拟场景中的各种实体统计信息来表征输入状态。这些数值特征包括，例如，不带插件的模型数量、带插件的模型数量、模型内的插件数量以及 world 节点下的插件数量。

对于每个模块，给定一个输入向量 $s$ ，具有 $N$ 个神经元的隐藏层使用权重矩阵 $W_1$​ 和偏置向量 $b_1$ ​。隐藏层输出计算如下：

$ h = ReLU (𝑊_1 · 𝑠 + 𝑏_1).$

从隐藏层到输出，行动者生成一个长度为 $m$ 的一维向量，其中 $m$ 表示可用的生成器选择的数量。该向量作为基于输入特征 $s$ 选择每个生成器的概率分布。权重矩阵 $W_2$ 的维数是 $m \times N$，而偏置向量 $b_2$ 的维数是 $m$。输出计算如下：

$ y_{𝑎𝑐𝑡𝑜𝑟} = Softmax (𝑊_2 · h + 𝑏_2).$

子行动者的工作方式与行动者类似，除了它们用于生成器的参数之外。对于评判者模块，输出表示当前迭代的估计奖励，使用权重矩阵 $W_3$ 和偏差标量 $b_3$。输出为：

$ y_{𝑐𝑟𝑖𝑡𝑖𝑐} = 𝑊_3 · h + 𝑏_3.$

在执行命令序列之后，行动者和评判者模块将被更新。更详细地说，总奖励是三个组成部分的总和：
- Crash 奖励是为了鼓励找到不同的 Crash 序列，其中 $\omega > 0$ 是一个参数，$n_t$ 表示在迭代 $t$ 之前触发相同崩溃类型的次数：

$$
𝑅_{𝑐𝑟𝑎𝑠ℎ}^t = 𝜔 × 𝑛𝑡
$$

- 覆盖率奖励鼓励增加代码覆盖率，其中 $\lambda > 0$ 是奖励覆盖率改进的参数：
  
$$
R_{cov}^t = 
\begin{cases}
\lambda, 覆盖率增加,\\
0, 其他，
\end{cases}
$$

- 多样性奖励用于鼓励命令生成器序列的多样性，通过将命令生成器序列 $c_t$ 与 $h$ 最近的序列进行比较来衡量其多样性 $div_t$ ：
  
$$
div_t = \frac{1}{|C_h |} \sum_{c_i ∈ C_h} 𝐷𝑖𝑠𝑡 (c_i, c_t )
$$

其中 $C_h = {c_{t-h}, \dots, c_{t-1}}$ 表示与当前序列 $c_t$ 最接近的一组指令生成序列，本研究中设定 $h = 100$。距离函数 $\text{Dist}(c_i, c_t) = 1 - \text{cosine}(V_i, V_t)$，其中 $V_i$ 和 $V_t$ 分别为 $c_i$ 和 $c_t$ 的向量表示，$\text{cosine}(V_i, V_t)$ 表示二者的余弦相似度。当前指令生成序列 $c_t$ 的多样性奖励定义为：

$$
R_{div}^t = \frac{1}{h} \sum_{i=1}^h (𝑑𝑖𝑣_{𝑡} − 𝑑𝑖𝑣_{𝑡−𝑖} )
$$

最终的奖励是：
$$
Reward (t) = 𝑅_{crash}^t + 𝑅_{cov}^t + 𝑅_{div}^t. 
$$

获得实际奖励后，GzFuzz 进一步使用评判者模块来获取预测的潜在奖励。受现有研究的启发，我们考虑使用优势损失函数来减少神经网络的高方差。更具体地说，对于评判者模块，损失使用均方误差计算：

$$
Advantage(𝑡) = Reward (𝑡) − y_{𝑐𝑟𝑖𝑡𝑖𝑐}, \\
valueLoss(𝑡) = Advantage(𝑡)^2
$$

其中 Reward(t)是实际奖励，ycritic​ 是评判者模块在迭代 t 时预测的奖励。对于行动者和子行动者，损失使用策略梯度方法更新：

$$
PolicyLoss(t)=−log(P_θ​(s,c_t​))⋅Advantage(t),
$$

其中 $P_\theta(s, c_t)$ 是在输入特征 $s$ 下选择 $c_t$ 的转移概率，它是由网络参数 $\theta$ 下对应的行动者/子行动者预测的。

行动者/子行动者和评判者模块都使用反向传播来更新参数，目标是最大化累积奖励。请注意，对于子行动者，只更新那些对应于所选命令生成器的子行动者：

$$
θ=UpdateParameter(θ,PolicyLoss(t),α),\\
ϕ=UpdateParameter(ϕ,ValueLoss(t),α),
$$

其中 $\theta$ 和 $\phi$ 分别表示行动者/子行动者和评判者模块的参数。在本研究中，我们利用 Adam 优化器来更新参数，$\alpha$ 表示神经网络模块的初始学习率。

对于上述参数，在下图中介绍了 GzFuzz 中引入的参数设置。$\omega$ 的值是根据现有研究设置的，GzFuzz 的其余参数设置基于一个小规模实验。在我们的初步实验中，我们观察到 GzFuzz 对参数设置不太敏感。

![](../images/learning/2025_issta_gzfuzz/param.png)

### 三、实例

下图展示了一个已修复的真实错误。具体来说，图 4(a) 显示了一个带有 `AckermannSteering` 插件的模型片段，而图 4(b) 演示了一个创建并插入此模型的 Gazebo 命令。图 4(c) 展示了一个通过调用服务 `/world/world_0/control/` 并附带消息 `"reset {all: true}"` 来重置模拟的命令。执行后，模拟重新启动，模型 vehicle_blue预计将从场景中移除。然而，由于底层实现的问题，`AckermannSteering` 插件未能正确卸载，这进一步导致模拟器崩溃。

![](../images/learning/2025_issta_gzfuzz/example.png)

为了触发此错误，基于学习的命令生成器选择首先通过行动者模块选择生成器 6 和 1。生成器 6 需要其对应的子行动者指定 `vehicle_blue` 模型作为参数，而生成器 1 选择 `world/world_@/control` 服务，并基于 Protobuf 定义构建一个重置消息。语法感知的可行命令生成机制确保两个命令都符合 Gazebo 的语法要求。执行这些命令会触发该错误，该错误源于 Gazebo 对级联插件移除的错误处理。

### 四、实验环境

GzFuzz 使用 Python 实现。神经网络模块使用 PyTorch实现。对于 XML 解析和修改，使用了 lxml 库 (5.3.0)。为了为给定的 Protobuf 定义生成可行消息，使用了 randomproto 库 (0.0.1)。至于硬件环境，我们的实验在一台运行 Ubuntu 22.04 (x86_64) 的 GNU/Linux PC 上进行，该 PC 配备 Intel Core i9-13900K CPU 和 128GB RAM。
